quant_diff = qnorm(1-(alpha/2))*sqrt(y*(n-y)/n^3)
upper = y/n+quant_diff
lower = y/n-quant_diff
structure(list(Upper = upper, Lower = lower))
}
conf_int(7, 20, .05)
conf_int(4, 20, .05)
###Quantile test###
##from : https://people.stat.sc.edu/hitchcock/Rexamples518section3_2.txt
quantile.test<-function(x,xstar=0,quantile=.5,alternative="two.sided"){
n<-length(x)
p<-quantile
T1<-sum(x<=xstar)
T2<-sum(x< xstar)
if (alternative=="quantile.less") {
p.value<-1-pbinom(T2-1,n,p)}
if (alternative=="quantile.greater"){
p.value<-pbinom(T1,n,p)}
if (alternative=="two.sided"){
p.value<-2*min(1-pbinom(T2-1,n,p),pbinom(T1,n,p))}
list(xstar=xstar,alternative=alternative,T1=T1,T2=T2,p.value=p.value)
}
#an example case for this test is, say there are students with their test score recorded.
#then we would like to inspect whether the quartile of the scores is 193
testscores <- c(189,233,195,160,212,176,231,185,199,213,202,193,174,166,248)
quantile.test(testscores,xstar=193,quantile=0.75,alternative="two.sided")
#another example is for testing one-tailed quantile.
#suppose we have prices for houses within a neighborhood, then we want to test whether the median
#of the house price is at least 179
prices <- c(120, 500, 64, 104, 172, 275, 336, 55, 535, 251, 214, 1250, 402, 27, 109, 17, 334, 205)
quantile.test(prices,xstar=179, quantile=0.5, alternative="quantile.less")
sort(prices)
##confidence interval for quantile
#to make the (1-alpha) confidence interval for certain quantile
quantile.interval<-function(x,quantile=.5,conf.level=.95){
n<-length(x)
p<-quantile
alpha<-1-conf.level
rmin1<-qbinom(alpha/2,n,p)-1
r<-rmin1+1
alpha1<-pbinom(r-1,n,p)
smin1<-qbinom(1-alpha/2,n,p)
s<-smin1+1
alpha2<-1-pbinom(s-1,n,p)
clo<-sort(x)[r]
chi<-sort(x)[s]
conf.level<-1-alpha1-alpha2
list(quantile=quantile,conf.level=conf.level,r=r,s=s,interval=c(clo,chi))}
#example with the house prices : 95% CI for median
quantile.interval(prices)
#example with the house prices : 95% CI for median
quantile.interval(prices, .5, .95)
#example with the house prices : 90% CI for .7 quantile
quantile.interval(prices, .7, .9)
##sign test
#in a sense sign test is simply another kind of binomial test. the importance to be remembered is
#this test, most of the time, is used for paired data. thus, it is also similar to t-test for paired samples
library(tidyverse)
library(rstatix)
library(ggpubr)
install.packages("datarium")
data(mice2, datarium)
data("mice2", package = datarium)
library(datarium)
data(mice2)
mice2
mice2 = mice2 %>%
gather(key = "group", value = "weight", before, after)
head(mice2, 3)
#summarize
mice2 = %>$
group_by(group)%>%
mean
View(mice2)
#summarize
mice2 %>$
group_by(group)%>%
mean
#summarize
mice2 %>$
group_by(group)%>%
mean(weight)
#summarize
mice2 %>$
group_by(group)%>%
mean(weight)
#summarize
mice2%>$group_by(group)%>%mean(weight)
#summarize
mice2%>$group_by(group)%>%summarise(mean_weight = mean(weight))
mice2 %>%
group_by(group) %>%
get_summary_stats(weight, type = "median_iqr")
#summarize
mice2 %>% group_by(group) %>% summarise(mean_weight = mean(weight))
#summarize
mice2 %>% group_by(group) %>% summarise(mean_weight = mean(weight),
median_weight = median(weight))
#summarize
mice2 %>% group_by(group) %>% summarise(n = n(weight),
mean_weight = mean(weight),
median_weight = median(weight))
n(mice)
n(prices)
count(prices)
summary(prices)
length(mice2$weight)
#summarize
mice2 %>% group_by(group) %>% summarise(n = length(weight),
mean_weight = mean(weight),
median_weight = median(weight))
#summarize
mice2 %>% group_by(group) %>% summarise(n = length(weight),
mean_weight = mean(weight),
median_weight = median(weight),
iqr = IQR(weight))
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before adn after treatment
#now we use the statistics test for sign test
stat.test = mice2 %>%
sign_test(weight ~ group) %>%
add_significance()
stat.test
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before adn after treatment
#now we use the statistics test for sign test
sign_test(weight ~ group)
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before adn after treatment
#now we use the statistics test for sign test
sign_test(mice2, weight ~ group)
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before adn after treatment
#now we use the statistics test for sign test
sign_test(mice2, weight ~ group, alternative = "right tail")
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before adn after treatment
#now we use the statistics test for sign test
sign_test(mice2, weight ~ group, alternative = "greater")
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before adn after treatment
#now we use the statistics test for sign test
sign_test(mice2, weight ~ group, alternative = "less")
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before adn after treatment
#now we use the statistics test for sign test
sign_test(mice2, weight ~ group, alternative = "greater")
##sign test exercise from chapter 3.4 W. J. Conover - Practical Nonparametric Statistics, 3rd (1999, Wiley)
# 2
binom.test(22,28)
##sign test exercise from chapter 3.4 W. J. Conover - Practical Nonparametric Statistics, 3rd (1999, Wiley)
# 2
binom.test(22,28, alternative = "greater")
##sign test exercise from chapter 3.4 W. J. Conover - Practical Nonparametric Statistics, 3rd (1999, Wiley)
# 2
binom.test(22,28, alternative = "less")
##sign test exercise from chapter 3.4 W. J. Conover - Practical Nonparametric Statistics, 3rd (1999, Wiley)
# 2
binom.test(22,28, alternative = "greater")
# 3
binom.test(77,100)
#reaction time after lunch being longer than reaction time before lunch
# 3
binom.test(77,100) #evidently there is significant difference of durability affected by two of the additives
#reaction time after lunch being longer than reaction time before lunch
# 3
binom.test(23,100) #evidently there is significant difference of durability affected by two of the additives
# 4
binom.test(7, 22)
# 4
binom.test(12, 22)
#based on the result above, it can be concluded that the values of weight from the specimens whom
#received treatment tend to be higher than the weight of the specimens whom has not received the treatment
binom.test(8,10, alternative = "greater")
#based on the result above, it can be concluded that the values of weight from the specimens whom
#received treatment tend to be higher than the weight of the specimens whom has not received the treatment
binom.test(8,9, alternative = "greater")
binom.test(82,82)
0.5^81
binom.test(8,10)
binom.test(9,10)
binom.test(8,10, alternative = "greater")
binom.test(9,10, alternative = "greater")
n(prices)
sum(prices)
count(prices)
library(plyr)
count(prices)
##sign test exercise from chapter 3.4 W. J. Conover - Practical Nonparametric Statistics, 3rd (1999, Wiley)
# 2
binom.test(22,26, alternative = "greater") #statistics test shows a significant evidence of
#reaction time after lunch being longer than reaction time before lunch
# 3
binom.test(23,100) #evidently there is significant difference of durability affected by two of the additives
#reaction time after lunch being longer than reaction time before lunch
# 3
binom.test(77,100) #evidently there is significant difference of durability affected by two of the additives
# 4
binom.test(12, 19) #there is not enough evidence of significant difference in preferences
# 4
binom.test(7, 19) #there is not enough evidence of significant difference in preferences
##mcnemar test
#variation from sign test in which instead of ordinal, we have categorical variables
#another way to see it is suppose we have bivariate variable (joint variables) independent with each other
#then for each of these individual variables has exactly 2 categories (lets call it 0 and 1)
#illustration : we have two surveys conducted asking whether choosing between 2 candidates of the election.
#the first survey conducted just before the candidates debate took place and the second one was held after the debate.
vote = matrix(c(144, 29, 31, 431), 2,2, byrow = T)
#after obtaining the data, we would like to conduct a test whether the debate has significant effect to
#people's vote or not :
mcnemar.test(vote)
vote
binom.test(29,60)
#summarize data
mice2 %>% group_by(group) %>% summarise(n = length(weight),
mean_weight = mean(weight),
median_weight = median(weight),
iqr = IQR(weight))
##sign test
#a test to see whether one variable tend to have higher values than the other variable.
#in a sense sign test is simply another kind of binomial test(p*=.5). the importance to be remembered is
#this test, most of the time, is used for paired data. thus, it is also similar to t-test for paired samples
library(tidyverse)
library(rstatix)
library(ggpubr)
library(datarium)
#summarize data
mice2 %>% group_by(group) %>% summarise(n = length(weight),
mean_weight = mean(weight),
median_weight = median(weight),
iqr = IQR(weight))
##wilcoxon test
#alternative to t-test
#recommended to apply on data that is not satisfying the normality assumption
#just like t-test, wilcoxon test also has several types. lets take a look at mice dataset again
#one sample
bxp <- ggboxplot(
mice$weight, width = 0.5, add = c("mean", "jitter"),
ylab = "Weight (g)", xlab = FALSE
)
bxp
wilcox.test(mice$weight, mu = 27)
wilcox.test(mice$weight, mu = 25)
wilcox.test(mice$weight, mu = 20)
wilcox.test(mice$weight, mu = 21)
wilcox.test(mice$weight, mu = 19)
wilcox.test(mice$weight, mu = 23)
#two sample
#using genderweight dataset from datarium
data("genderweight", package = "datarium")
genderweight
bxp2 = ggboxplot(
genderweight, x = "group", y = "weight", xlab = "Gender", ylab = "weight",
width = 0.5, add = c("mean", "jitter"),
)
bxp2
wilcox.test(genderweight, weight~group)
wilcox.test(genderweight$weight ~ genderweight$group)
genderweight %>% group_by(group) %>% summarise(n = length(weight),
mean_weight = mean(weight),
median_weight = median(weight),
iqr = IQR(weight))
#signed rank on paired sample
mice2
#signed rank on paired sample
mice2 %>% gather(key = "group", value = "weight", before, after)
#signed rank on paired sample
miceff = mice2 %>% gather(key = "group", value = "weight", before, after)
#signed rank on paired sample
head(mice2, 3)
#signed rank on paired sample
data("mice2")
miceff = mice2 %>% gather(key = "group", value = "weight", before, after)
micef
miceff
wilcox_test(miceff$weight ~ miceff$group, paired = TRUE)
wilcox.test(miceff$weight ~ miceff$group, paired = TRUE)
#to evaluate mean differences, after each shuffle, the means computed from the shuffled data are
#compared with the observed mean difference.
#illustration : ecologist's interest in understanding the factors that shape the organization
#of ecological communities. Are the species in an ecological community just a random assortment of species
#available from the regional species pool? Conversely, do species interactions and shared resources
#determine the local distribution of species, such that some species are found together more often
#than we would expect by chance whereas other never or only rarely co-occur.
#The idea that competition and shared resources are important in driving community assembly
#is known as assembly rules and was first proposed by Diamond (1975).
#lets look at the actual application, starting with the data
comMatrix_example = "https://uoftcoders.github.io/rcourse/data/lec09_CommunityMatrix_Example.csv"
download.file(comMatrix_example, "lec09_CommunityMatrix_Example.csv")
comMatrix_example = read_csv("lec09_CommunityMatrix_Example.csv")
head(comMatrix_example)
colSums(comMatrix_example[,2:5])
#to evaluate mean differences, after each shuffle, the means computed from the shuffled data are
#compared with the observed mean difference.
#illustration : ecologist's interest in understanding the factors that shape the organization
#of ecological communities. Are the species in an ecological community just a random assortment of species
#available from the regional species pool? Conversely, do species interactions and shared resources
#determine the local distribution of species, such that some species are found together more often
#than we would expect by chance whereas other never or only rarely co-occur.
#The idea that competition and shared resources are important in driving community assembly
#is known as assembly rules and was first proposed by Diamond (1975).
#lets look at the actual application, starting with the data
neon_data = "https://uoftcoders.github.io/rcourse/data/NEON_PlantPA_HARV_201707.csv"
download.file(neon_data, "NEON_PlantPA_HARV_201707.csv")
neon_data = read_csv("NEON_PlantPA_HARV_201707.csv")
#change into presence-absence matrix
neon_data_filtered <- neon_data %>%
# We only want rows with plant species and not 'otherVariables'
dplyr::filter(divDataType == "plantSpecies") %>%
# To create a presence-absence matrix, we only need the taxonID
# (i.e. species) and the plotID (i.e. Site)
dplyr::select(plotID, taxonID)
# Keep only unique rows so that cells represent presence-absence
# and not abundances
neon_data_filtered = unique(neon_data_filtered)
PA_matrix = dcast(neon_data_filtered, formula = taxonID ~ plotID, fun.aggregate = length)
library(reshape2)
library(EcoSimR)
install.packages("ecosim")
library(ecosim)
PA_matrix = dcast(neon_data_filtered, formula = taxonID ~ plotID, fun.aggregate = length)
head(PA_matrix)
#cooccurence analysis
set.seed(50)
co_oc_analysis <- cooc_null_model(PA_matrix, algo = "sim2",
nReps=1000,
metric = "c_score",
suppressProg = TRUE,
saveSeed = TRUE)
install.packages('EcoSimR')
version
update()
update(R)
###Tests based on Binomial distribution###
##exact binomial test method using stats library
library(stats)
library(tidyverse)
library(rstatix)
library(ggpubr)
library(datarium)
#we need a vector showing number of (success, failure) or number of success and total trial
databin = c(10233, 69777)
#we are to test whether the success rate is p* = 0.125
binom.test(databin, p = 0.125)
binom.test(10233, 10233+69777, 0.125) #two-tailed
binom.test(databin, p = 0.125, alternative = "greater") #right-tailed
#problem example from conover book (1999) : in an effort to reduce the side effects from prostate cancer
#operation which are usually being experienced by at least half of the patients,
#suppose there is a new method of operation. from 19 trials, 3 people experienced unpleasant side effects.
#is it safe to conclude that the new method is effective on reducing the side effects from the operation?
#this problem can be viewed as a binomial test problem which alternative hypothesis is left-tailed
#and probability of .5
binom.test(3, 19, 0.5, alternative = "less") #it can be concluded that the new method is effective
#problem example from conover book (1999) : in an effort to reduce the side effects from prostate cancer
#operation which are usually being experienced by at least half of the patients,
#suppose there is a new method of operation. from 19 trials, 3 people experienced unpleasant side effects.
#is it safe to conclude that the new method is effective on reducing the side effects from the operation?
#this problem can be viewed as a binomial test problem which alternative hypothesis is left-tailed
#and probability of .5
binom.test(16, 19, 0.5, alternative = "less") #it can be concluded that the new method is effective
#problem example from conover book (1999) : in an effort to reduce the side effects from prostate cancer
#operation which are usually being experienced by at least half of the patients,
#suppose there is a new method of operation. from 19 trials, 3 people experienced unpleasant side effects.
#is it safe to conclude that the new method is effective on reducing the side effects from the operation?
#this problem can be viewed as a binomial test problem which alternative hypothesis is left-tailed
#and probability of .5
binom.test(16, 19, 0.5, alternative = "greater") #it can be concluded that the new method is effective
##using approximation with normal distribution
#test statistic using quantile based on normal distribution
quantil = function(n,p,q){
nr <- round(n)
if (any(is.na(n) | (n < 0)) || max(abs(n - nr)) > 1e-07)
stop("'n' must be nonnegative and integer")
n <- nr
if (!missing(p) && (length(p) > 1L || is.na(p) || p < 0 ||
p > 1))
stop("'p' must be a single number between 0 and 1")
if (!missing(q) && (length(q) > 1L || is.na(q) || q < 0 ||
q > 1))
stop("'q' must be a single number between 0 and 1")
quant = n * p + qnorm(q)*sqrt(n*p*(1-p))
structure(quant)
}
quantil(19.6,0.5,0.05)
quantil(19,0.5,0.05)
#suppose we want to test whether a certain type of genotype on a plant has the chance of .75 to occurs
#the data consist of genotype "a" appeared on 243 plants while the "b" genotype appeared on 682 plants.
#test is conducted to test for "b" genotype having the probability of .75 to appear on the species of the plant
binom.test(682, 682+243, .75)
quantil(925, .75, .025)#lower quantile
quantil(925, .75, .975)#upper quantile
#using the above quantile, the critical region is 667.94 > T and T >  719.56
#pvalue from normal distribution approximation can be derived as
pnorm((682-.75*925+.5)/sqrt(925*.75*.25))
#confidence interval for probability or proportion of population
conf_int = function(y,n,alpha){
DNAME <- deparse1(substitute(y))
yr <- round(y)
if (any(is.na(y) | (y < 0)) || max(abs(y - yr)) > 1e-07)
stop("'y' must be nonnegative and integer")
y <- yr
if (length(y) == 2L) {
n <- sum(y)
y <- y[1L]
}
else if (length(y) == 1L) {
nr <- round(n)
if ((length(n) > 1L) || is.na(n) || (n < 1) || abs(n -
nr) > 1e-07 || (y > nr))
stop("'n' must be a positive integer >= 'y'")
DNAME <- paste(DNAME, "and", deparse1(substitute(n)))
n <- nr
}
else stop("incorrect length of 'y'")
if (!missing(alpha) && (length(alpha) > 1L || is.na(alpha) || alpha < 0 ||
alpha > 1))
stop("'alpha' must be a single number between 0 and 1")
quant_diff = qnorm(1-(alpha/2))*sqrt(y*(n-y)/n^3)
upper = y/n+quant_diff
lower = y/n-quant_diff
structure(list(Upper = upper, Lower = lower))
}
conf_int(4, 20, .05)
##Quantile test
#from : https://people.stat.sc.edu/hitchcock/Rexamples518section3_2.txt#
quantile.test<-function(x,xstar=0,quantile=.5,alternative="two.sided"){
n<-length(x)
p<-quantile
T1<-sum(x<=xstar)
T2<-sum(x< xstar)
if (alternative=="quantile.less") {
p.value<-1-pbinom(T2-1,n,p)}
if (alternative=="quantile.greater"){
p.value<-pbinom(T1,n,p)}
if (alternative=="two.sided"){
p.value<-2*min(1-pbinom(T2-1,n,p),pbinom(T1,n,p))}
list(xstar=xstar,alternative=alternative,T1=T1,T2=T2,p.value=p.value)
}
#an example case for this test is, say there are students with their test score recorded.
#then we would like to inspect whether the upper quartile of the scores is 193
testscores <- c(189,233,195,160,212,176,231,185,199,213,202,193,174,166,248)
quantile.test(testscores,xstar=193,quantile=0.75,alternative="two.sided")
#another example is for testing one-tailed quantile.
#suppose we have prices for houses within a neighborhood, then we want to test whether the median
#of the house price is at least 179
prices <- c(120, 500, 64, 104, 172, 275, 336, 55, 535, 251, 214, 1250, 402, 27, 109, 17, 334, 205)
quantile.test(prices,xstar=179, quantile=0.5, alternative="quantile.less")
sort(prices)
##confidence interval for quantile
#to make the (1-alpha) confidence interval for certain quantile
quantile.interval<-function(x,quantile=.5,conf.level=.95){
n<-length(x)
p<-quantile
alpha<-1-conf.level
rmin1<-qbinom(alpha/2,n,p)-1
r<-rmin1+1
alpha1<-pbinom(r-1,n,p)
smin1<-qbinom(1-alpha/2,n,p)
s<-smin1+1
alpha2<-1-pbinom(s-1,n,p)
clo<-sort(x)[r]
chi<-sort(x)[s]
conf.level<-1-alpha1-alpha2
list(quantile=quantile,conf.level=conf.level,r=r,s=s,interval=c(clo,chi))}
#example with the house prices : 95% CI for median
quantile.interval(prices, .5, .95)
#example with the house prices : 90% CI for .7 quantile
quantile.interval(prices, .7, .9)
##sign test
#a test to see whether one variable tend to have higher values than the other variable.
#in a sense sign test is simply another kind of binomial test(p*=.5). the importance to be remembered is
#this test, most of the time, is used for paired data. thus, it is also similar to t-test for paired samples
data(mice2)
mice2 #from datarium library
#transform data by gathering the value of "before" and "after" in a column
mice2 = mice2 %>%
gather(key = "group", value = "weight", before, after)
#summarize data
mice2 %>% group_by(group) %>% summarise(n = length(weight),
mean_weight = mean(weight),
median_weight = median(weight),
iqr = IQR(weight))
#based on the above summary, it can be seen the big difference in mean and median values for the
#weight of specimens before and after treatment
#now we use the statistics test for sign test
mice2
sign_test(mice2, weight ~ group, alternative = "greater")
#based on the result above, it can be concluded that the values of weight from the specimens whom
#received treatment tend to be higher than the weight of the specimens whom has not received the treatment
binom.test(8,9, alternative = "greater")
binom.test(82,82)
binom.test(9,10, alternative = "greater")
##sign test exercise from chapter 3.4 W. J. Conover - Practical Nonparametric Statistics, 3rd (1999, Wiley)
# 2
binom.test(22,26, alternative = "greater") #statistics test shows a significant evidence of
#reaction time after lunch being longer than reaction time before lunch
# 3
binom.test(77,100) #evidently there is significant difference of durability affected by two of the additives
# 4
binom.test(7, 19) #there is not enough evidence of significant difference in preferences
##mcnemar test(test of change significance)
#variation from sign test in which instead of ordinal, we have categorical variables
#another way to see it is suppose we have bivariate variable (joint variables) independent with each other
#then for each of these individual variables has exactly 2 categories (lets call it 0 and 1)
#illustration : we have two surveys conducted asking whether choosing between 2 candidates of the election.
#the first survey conducted just before the candidates debate took place and the second one was held after the debate.
vote = matrix(c(144, 29, 31, 431), 2,2, byrow = T)
vote
#after obtaining the data, we would like to conduct a test whether the debate has significant effect to
#people's vote or not :
mcnemar.test(vote)
binom.test(29,60)
